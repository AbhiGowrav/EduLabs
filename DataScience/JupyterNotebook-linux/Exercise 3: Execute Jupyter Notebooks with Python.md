# Execute Jupyter Notebooks with Python

## Overview

In this task you will learn about:

   - Torch.Autograd
   - Tensors, Functions and Computational graph
   - Computing Gradients
   - Disabling Gradient Tracking
   - More on Computational Graphs
   - Optional Reading: Tensor Gradients and Jacobian Products

### Task 1: Execute Jupyter Notebooks with Python

1. In the Jupyter lab, in the left pane, Navigate to **Pytorch** folder.

   ![](../images/pytorch.png)

1. Inside the Pytorch folder, Navigate to **Beginner -> Basics** folder.

   ![](../images/beginnerfolder.png)
   
   ![](../images/basicfolder.png)

1. Under the **Basics** folder, Select **autograd_tutorial.html** file to execute in the right side of JupyterLab portal.

   ![](../images/auto.png)
   
1. Execute each cell one at a time by clicking on it and selecting the execute button.

   ![](../images/execute.png)
   
   **Note** : You can run the notebook document step-by-step (one cell a time) by pressing crtl + enter for running the particular cell or shift + enter to run the current cell.

## Summary

In this exercise, you have learnt that PyTorch has a built-in differentiation engine called torch.autograd. It supports automatic computation of gradient for any computational graph and Computing Gradients is used to optimize weights of parameters in the neural network, we need to compute the derivatives of our loss function with respect to parameters. 
